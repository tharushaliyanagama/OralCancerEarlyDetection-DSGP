{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-KlHoLpgiKKiFcs_7Db2E0uSAO5kRmr4",
      "authorship_tag": "ABX9TyNJ0CmjQnKnh2pfP+pc7vyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tharushaliyanagama/OralCancerEarlyDetection-DSGP/blob/Image-Validation/Validation_DSGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X-XQDYOAIPJ4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "import random\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAxA-yLJJJeb",
        "outputId": "859b691a-bcc1-451c-f6e2-d63f5c33cc01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "main_dataset_dir = \"/content/drive/MyDrive/Train\"\n",
        "output_dir = \"/content/drive/MyDrive/Divided_DSGP\"\n",
        "train_split = 0.8         # 80% for training, 20% for validation\n",
        "\n",
        "# Create train/val folders\n",
        "os.makedirs(os.path.join(output_dir, \"train/mouth\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"train/non_mouth\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"val/mouth\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"val/non_mouth\"), exist_ok=True)"
      ],
      "metadata": {
        "id": "6QyJYRccyktc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split data\n",
        "def split_data(class_name):\n",
        "    class_dir = os.path.join(main_dataset_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "    random.shuffle(images)\n",
        "    split_index = int(len(images) * train_split)\n",
        "\n",
        "    train_images = images[:split_index]\n",
        "    val_images = images[split_index:]\n",
        "\n",
        "    # Move files to train and val folders\n",
        "    for img in train_images:\n",
        "        src = os.path.join(class_dir, img)\n",
        "        dst = os.path.join(output_dir, \"train\", class_name, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    for img in val_images:\n",
        "        src = os.path.join(class_dir, img)\n",
        "        dst = os.path.join(output_dir, \"val\", class_name, img)\n",
        "        shutil.copy(src, dst)"
      ],
      "metadata": {
        "id": "7BEyatT0ypYw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split for \"mouth\" and \"non_mouth\"\n",
        "split_data(\"mouth\")\n",
        "split_data(\"non_mouth\")\n",
        "\n",
        "print(\"Dataset split into training and validation sets!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnWuFx4ysVp",
        "outputId": "206656fb-42f8-4caa-cbd1-21f2fc5f6134"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into training and validation sets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your dataset\n",
        "data_dir = \"/content/drive/MyDrive/Divided_DSGP\"\n",
        "train_dir = os.path.join(data_dir, \"train\")  # Replace with your dataset's train folder\n",
        "val_dir = os.path.join(data_dir, \"val\")      # Replace with your dataset's validation folder"
      ],
      "metadata": {
        "id": "9FbOSTrQIlFb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLsWOt0DItWq",
        "outputId": "9a0fc178-8697-4f2d-ba7f-00144e7bcd98"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2603 images belonging to 2 classes.\n",
            "Found 651 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained model (MobileNetV2)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification: Mouth / Not Mouth\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlWKiJVwIzbI",
        "outputId": "64400e1e-0ee0-4e14-866c-386dff290e08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"mouth_detection_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAWebmcfI4FM",
        "outputId": "0219b076-83cf-4b15-fb9d-18af093a827b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m54/82\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.8612 - loss: 0.3575"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 2s/step - accuracy: 0.8737 - loss: 0.3228 - val_accuracy: 0.9478 - val_loss: 0.1411\n",
            "Epoch 2/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.9382 - loss: 0.1605 - val_accuracy: 0.9585 - val_loss: 0.1436\n",
            "Epoch 3/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 4s/step - accuracy: 0.9478 - loss: 0.1403 - val_accuracy: 0.9585 - val_loss: 0.1114\n",
            "Epoch 4/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 2s/step - accuracy: 0.9524 - loss: 0.1403 - val_accuracy: 0.9601 - val_loss: 0.0996\n",
            "Epoch 5/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 3s/step - accuracy: 0.9584 - loss: 0.1221 - val_accuracy: 0.9585 - val_loss: 0.1005\n",
            "Epoch 6/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 3s/step - accuracy: 0.9575 - loss: 0.1157 - val_accuracy: 0.9601 - val_loss: 0.1087\n",
            "Epoch 7/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.9638 - loss: 0.0997 - val_accuracy: 0.9677 - val_loss: 0.1044\n",
            "Epoch 8/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2s/step - accuracy: 0.9554 - loss: 0.0984 - val_accuracy: 0.9570 - val_loss: 0.1136\n",
            "Epoch 9/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.9664 - loss: 0.0929 - val_accuracy: 0.9631 - val_loss: 0.0916\n",
            "Epoch 10/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 3s/step - accuracy: 0.9652 - loss: 0.1013 - val_accuracy: 0.9647 - val_loss: 0.0916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate an image\n",
        "def validate_image(image_path, model_path=\"mouth_detection_model.h5\"):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    image = load_img(image_path, target_size=(img_height, img_width))\n",
        "    image_array = img_to_array(image) / 255.0\n",
        "    image_array = tf.expand_dims(image_array, axis=0)  # Add batch dimension\n",
        "    prediction = model.predict(image_array)[0][0]\n",
        "    if prediction > 0.5:\n",
        "        return \"Inside of a mouth\"\n",
        "    else:\n",
        "        return \"Not inside of a mouth\""
      ],
      "metadata": {
        "id": "iPpQZQQCI6SF"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}